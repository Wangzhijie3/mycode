### Logistic回归

首先我们要先了解sigmoid函数，sigmoid函数是可以将任何实数投射到[0,1]区间，从而方便我们做事情：

![欧式距离](../images/../../images/log1)

我们预测值代入进去：

![欧式距离](../../images/log3)

对h(x)求偏导：

![欧式距离](../../images/log4)

最终的结果还是满漂亮的，现在我们假定：

![欧式距离](../../images/log5)

将求导结果代入：

![欧式距离](../../images/log6)

其中倒第二步，是求所有样本的联合概率，我们对其取对数求对数似然：

![欧式距离](../../images/log7)

对θ求导：

![欧式距离](../../images/log8)

从结果看，我们会发现和线性回归的求到结果一样，Logistic回归参数学习过程：

![欧式距离](../../images/log9)

当然了，既然是梯度下降，我们可以选择BGD、SGD、mini-batch GD来做.

我们不妨求一下事件发生的几率(发生的概率与不发生的比值)：

![欧式距离](../../images/log10)

由此我们可以看得出，之所以把Logistic也划归为线性回归，是因为Logistic是对数线性模型。

当然了，我们也可以写一下Logistic的损失函数，得到对数似然：

![欧式距离](../../images/log11)

进一步推导：

![欧式距离](../../images/log12)

进一步推导，得到NLL(负对数似然)：

![欧式距离](../../images/log13)

当然了，有的教材上y的取值为{-1,1}，那么损失函数会有一些变化，但是思路是一样的，就不赘述了。

我们可以将Logistic推广到多分类，也就是Softmax回归。

假设第k类的参数为θk，组成二维矩阵θk×n，概率可以如下表示：

![欧式距离](../../images/log14)

似然函数和对数似然：

![欧式距离](../../images/log15)

随机梯度：

![欧式距离](../../images/log16)




