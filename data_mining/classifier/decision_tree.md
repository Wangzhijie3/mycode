## 决策树

树是一种极其重要的数据结构，像二叉树、红黑树等等，本要介绍的这种树是机器学习中的一种树，用来做分类或者回归的决策树。

![欧式距离](../../images/dt1)

上图就是两颗决策树，其中的每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表着一种类别。

决策树是以实例为基础的归纳学习，决策树学习是采用自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子结点处熵值为0，此时每个叶子结点上的实例都属于同一类。

决策树学习算法最大的优点是，他可以自学习，在学习过程中，不需要使用者了解过多的背景知识、领域知识，只需要对训练实例进行较好的标注就可以自学习了。

建立决策树的关键在于当前状态下选择哪一个属性作为分类依据，根据不同的目标函数，有三种主要的算法：
- ID3(Iterative Dichotomiser)
- C4.5
- CART(Classification And Regression Tree)

后边我们将一一介绍三种算法，现在先来了解几个概念。

#### 经验熵：
当熵和条件熵中的概率由数据统计(特别是极大似然估计)得到时，所对应的熵和条件熵分别称为经验熵、经验条件熵。

#### 信息增益：

表示因为特征A的信息而得知类D的信息的不确定性减少程度。可以如下表示：

![欧式距离](../../images/dt2)

当然由公式也可以看出，其实就是D与A的互信息。

### ID3

首先我们先约定记号：
- 设训练数据集为D，|D|为样本个数。
- 设有K个分类Ck， k=1，2...K，|Ck|表示属于Ck分类的样本个数，因此有：
  
  ![欧式距离](../../images/dt3)
- 设特征A有n个不同的取值{a1，a2 ... an}，根据特征A的取值将D划分为n个子集D1，D2 ... Dn，|Di|为Di子集中的样本个数，因此有： 
  
  ![欧式距离](../../images/dt4)
- 记子集Di中属于类Ck的样本的集合为Dik，|Dik|表示Dik的样本个数。

计算数据集D的经验熵：

![欧式距离](../../images/dt5)

接下来就要求经验条件熵了，可以遍历A中的所有特征，计算所有特征A的信息增益，选择信息增益最大的特征作为当前的分裂特征：

![欧式距离](../../images/dt6)

理解这块的关键我个人觉得有两点：第一时要理解决策树每一步是怎么得到的，可以手动做一棵来加深理解，第二点就是要记住记号的含义是干嘛的。这就是ID3

### C4.5

我们在ID 3的算法中不是求了信息增益么，只要再除以一个特征的经验熵就得到信息增益率：

![欧式距离](../../images/dt7)

